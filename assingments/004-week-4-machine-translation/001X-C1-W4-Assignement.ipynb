{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Machine Translation and Local Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "\n",
    "from utils import cosine_similarity, get_dict, process_tweet\n",
    "from os import getcwd\n",
    "import w4_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The embeddings as a dict - key = word, value=embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"./data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:57: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n",
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:57: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    }
   ],
   "source": [
    "en_fr_train = get_dict(\"./data/en-fr.train.txt\")\n",
    "en_fr_test = get_dict(\"./data/en-fr.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 'la',\n",
       " 'and': 'et',\n",
       " 'was': 'était',\n",
       " 'for': 'pour',\n",
       " 'that': 'cela',\n",
       " 'with': 'avec',\n",
       " 'from': 'depuis',\n",
       " 'this': 'ce',\n",
       " 'utc': 'tuc',\n",
       " 'his': 'son',\n",
       " 'not': 'pas',\n",
       " 'are': 'sont',\n",
       " 'talk': 'parlez',\n",
       " 'which': 'lequel',\n",
       " 'also': 'egalement',\n",
       " 'were': 'étaient',\n",
       " 'but': 'mais',\n",
       " 'have': 'ont',\n",
       " 'one': 'one',\n",
       " 'new': 'nouveautés',\n",
       " 'first': 'premiers',\n",
       " 'page': 'page',\n",
       " 'you': 'you',\n",
       " 'they': 'eux',\n",
       " 'had': 'avais',\n",
       " 'article': 'article',\n",
       " 'who': 'who',\n",
       " 'all': 'all',\n",
       " 'their': 'leurs',\n",
       " 'there': 'là',\n",
       " 'made': 'fabriqué',\n",
       " 'its': 'son',\n",
       " 'people': 'personnes',\n",
       " 'may': 'peut',\n",
       " 'after': 'aprés',\n",
       " 'other': 'autres',\n",
       " 'should': 'devrais',\n",
       " 'two': 'deux',\n",
       " 'score': 'partition',\n",
       " 'her': 'her',\n",
       " 'can': 'peut',\n",
       " 'would': 'ferait',\n",
       " 'more': 'plus',\n",
       " 'she': 'elle',\n",
       " 'when': 'quand',\n",
       " 'time': 'heure',\n",
       " 'team': 'equipe',\n",
       " 'american': 'américains',\n",
       " 'such': 'telles',\n",
       " 'discussion': 'débat',\n",
       " 'links': 'liens',\n",
       " 'only': 'seule',\n",
       " 'some': 'quelques',\n",
       " 'see': 'vois',\n",
       " 'united': 'unies',\n",
       " 'years': 'ans',\n",
       " 'school': 'école',\n",
       " 'world': 'mondiale',\n",
       " 'university': 'universitaire',\n",
       " 'during': 'lors',\n",
       " 'out': 'out',\n",
       " 'state': 'état',\n",
       " 'states': 'états',\n",
       " 'national': 'nationales',\n",
       " 'wikipedia': 'wikipedia',\n",
       " 'year': 'année',\n",
       " 'most': 'most',\n",
       " 'city': 'villes',\n",
       " 'used': 'utilisée',\n",
       " 'then': 'puis',\n",
       " 'county': 'comté',\n",
       " 'external': 'externes',\n",
       " 'where': 'où',\n",
       " 'will': 'sera',\n",
       " 'what': 'quelle',\n",
       " 'delete': 'effacer',\n",
       " 'these': 'ces',\n",
       " 'january': 'janvier',\n",
       " 'march': 'mars',\n",
       " 'august': 'août',\n",
       " 'july': 'juillet',\n",
       " 'being': 'être',\n",
       " 'film': 'film',\n",
       " 'him': 'lui',\n",
       " 'many': 'plusieurs',\n",
       " 'south': 'sud',\n",
       " 'september': 'septembre',\n",
       " 'like': 'aimez',\n",
       " 'between': 'entre',\n",
       " 'october': 'octobre',\n",
       " 'three': 'three',\n",
       " 'june': 'juin',\n",
       " 'well': 'bah',\n",
       " 'use': 'utilisez',\n",
       " 'war': 'war',\n",
       " 'under': 'under',\n",
       " 'them': 'eux',\n",
       " 'april': 'avril',\n",
       " 'born': 'born',\n",
       " 'december': 'decembre',\n",
       " 'link': 'lien',\n",
       " 'later': 'ultérieur',\n",
       " 'part': 'partie',\n",
       " 'november': 'novembre',\n",
       " 'players': 'joueurs',\n",
       " 'list': 'listes',\n",
       " 'please': 'svp',\n",
       " 'following': 'suivant',\n",
       " 'february': 'février',\n",
       " 'known': 'connu',\n",
       " 'second': 'seconde',\n",
       " 'name': 'noms',\n",
       " 'group': 'groupe',\n",
       " 'history': 'historique',\n",
       " 'series': 'séries',\n",
       " 'just': 'juste',\n",
       " 'north': 'nord',\n",
       " 'work': 'travailler',\n",
       " 'before': 'avant',\n",
       " 'since': 'puisque',\n",
       " 'season': 'saisons',\n",
       " 'both': 'both',\n",
       " 'high': 'élevé',\n",
       " 'through': 'via',\n",
       " 'district': 'district',\n",
       " 'now': 'maintenant',\n",
       " 'comments': 'observations',\n",
       " 'because': 'parceque',\n",
       " 'football': 'football',\n",
       " 'music': 'musique',\n",
       " 'however': 'cependant',\n",
       " 'diff': 'diff',\n",
       " 'century': 'century',\n",
       " 'league': 'ligue',\n",
       " 'edits': 'modifications',\n",
       " 'debate': 'débat',\n",
       " 'title': 'titre',\n",
       " 'articles': 'articles',\n",
       " 'john': 'john',\n",
       " 'same': 'même',\n",
       " 'including': 'comprenant',\n",
       " 'could': 'pourraient',\n",
       " 'english': 'anglais',\n",
       " 'album': 'album',\n",
       " 'number': 'numéro',\n",
       " 'against': 'against',\n",
       " 'family': 'familles',\n",
       " 'user': 'usager',\n",
       " 'based': 'basé',\n",
       " 'area': 'domaine',\n",
       " 'became': 'devint',\n",
       " 'york': 'york',\n",
       " 'life': 'vie',\n",
       " 'british': 'britannique',\n",
       " 'international': 'internationale',\n",
       " 'game': 'jeu',\n",
       " 'club': 'club',\n",
       " 'your': 'vos',\n",
       " 'early': 'tôt',\n",
       " 'best': 'meilleurs',\n",
       " 'west': 'west',\n",
       " 'house': 'maison',\n",
       " 'company': 'société',\n",
       " 'general': 'général',\n",
       " 'left': 'gauche',\n",
       " 'very': 'trés',\n",
       " 'here': 'voici',\n",
       " 'don': 'don',\n",
       " 'living': 'vivre',\n",
       " 'day': 'journee',\n",
       " 'several': 'plusieurs',\n",
       " 'place': 'lieu',\n",
       " 'party': 'fête',\n",
       " 'college': 'université',\n",
       " 'result': 'résultat',\n",
       " 'keep': 'conserver',\n",
       " 'appropriate': 'appropriée',\n",
       " 'four': 'quatre',\n",
       " 'even': 'même',\n",
       " 'class': 'classe',\n",
       " 'government': 'gouvernements',\n",
       " 'how': 'comment',\n",
       " 'called': 'appelés',\n",
       " 'did': 'did',\n",
       " 'each': 'chacun',\n",
       " 'found': 'trouvés',\n",
       " 'center': 'centre',\n",
       " 'per': 'per',\n",
       " 'style': 'style',\n",
       " 'com': 'com',\n",
       " 'long': 'long',\n",
       " 'country': 'pays',\n",
       " 'back': 'revenir',\n",
       " 'way': 'way',\n",
       " 'www': 'www',\n",
       " 'modify': 'modifier',\n",
       " 'end': 'fin',\n",
       " 'make': 'faire',\n",
       " 'public': 'publique',\n",
       " 'played': 'joué',\n",
       " 'won': 'gagnés',\n",
       " 'another': 'another',\n",
       " 'released': 'libéré',\n",
       " 'added': 'ajoutée',\n",
       " 'support': 'appui',\n",
       " 'games': 'jeux',\n",
       " 'former': 'ancienne',\n",
       " 'those': 'ceux',\n",
       " 'films': 'films',\n",
       " 'church': 'eglise',\n",
       " 'east': 'orient',\n",
       " 'line': 'line',\n",
       " 'major': 'majeur',\n",
       " 'members': 'adhérents',\n",
       " 'good': 'bonnes',\n",
       " 'much': 'beaucoup',\n",
       " 'image': 'image',\n",
       " 'show': 'spectacle',\n",
       " 'still': 'toujours',\n",
       " 'think': 'réfléchir',\n",
       " 'below': 'dessous',\n",
       " 'town': 'ville',\n",
       " 'last': 'dernières',\n",
       " 'system': 'système',\n",
       " 'right': 'droit',\n",
       " 'song': 'chanson',\n",
       " 'notable': 'remarquables',\n",
       " 'section': 'section',\n",
       " 'single': 'célibataires',\n",
       " 'included': 'compris',\n",
       " 'align': 'alignement',\n",
       " 'home': 'accueil',\n",
       " 'women': 'femme',\n",
       " 'television': 'télé',\n",
       " 'seed': 'semence',\n",
       " 'member': 'membre',\n",
       " 'goals': 'objectifs',\n",
       " 'sources': 'sources',\n",
       " 'book': 'réserver',\n",
       " 'station': 'gare',\n",
       " 'order': 'commander',\n",
       " 'old': 'vieille',\n",
       " 'information': 'information',\n",
       " 'set': 'définir',\n",
       " 'own': 'posséder',\n",
       " 'text': 'texte',\n",
       " 'band': 'bande',\n",
       " 'point': 'point',\n",
       " 'local': 'locaux',\n",
       " 'around': 'alentour',\n",
       " 'river': 'rivière',\n",
       " 'top': 'haut',\n",
       " 'main': 'principaux',\n",
       " 'language': 'langues',\n",
       " 'french': 'françaises',\n",
       " 'https': 'https',\n",
       " 'named': 'nommés',\n",
       " 'off': 'hors',\n",
       " 'note': 'notez',\n",
       " 'career': 'carrière',\n",
       " 'original': 'originaux',\n",
       " 'age': 'âges',\n",
       " 'service': 'service',\n",
       " 'established': 'établis',\n",
       " 'located': 'situé',\n",
       " 'said': 'disait',\n",
       " 'website': 'site',\n",
       " 'population': 'population',\n",
       " 'air': 'air',\n",
       " 'german': 'allemande',\n",
       " 'law': 'droit',\n",
       " 'military': 'militaires',\n",
       " 'great': 'grand',\n",
       " 'clubs': 'clubs',\n",
       " 'published': 'publié',\n",
       " 'president': 'président',\n",
       " 'park': 'parc',\n",
       " 'official': 'officiel',\n",
       " 'case': 'affaire',\n",
       " 'london': 'londres',\n",
       " 'times': 'fois',\n",
       " 'although': 'quoique',\n",
       " 'small': 'petite',\n",
       " 'third': 'troisièmement',\n",
       " 'different': 'différent',\n",
       " 'due': 'dû',\n",
       " 'get': 'obtenir',\n",
       " 'village': 'village',\n",
       " 'closed': 'clos',\n",
       " 'art': 'artistique',\n",
       " 'player': 'lecteur',\n",
       " 'final': 'définitive',\n",
       " 'community': 'collectivité',\n",
       " 'held': 'tenu',\n",
       " 'again': 'encore',\n",
       " 'began': 'commencé',\n",
       " 'army': 'armée',\n",
       " 'award': 'récompense',\n",
       " 'without': 'sans',\n",
       " 'death': 'mort',\n",
       " 'built': 'construits',\n",
       " 'men': 'homme',\n",
       " 'large': 'grand',\n",
       " 'site': 'site',\n",
       " 'using': 'utilisant',\n",
       " 'deletion': 'suppression',\n",
       " 'white': 'blanc',\n",
       " 'five': 'cinq',\n",
       " 'central': 'centrale',\n",
       " 'road': 'chemin',\n",
       " 'children': 'enfant',\n",
       " 'free': 'libre',\n",
       " 'took': 'prit',\n",
       " 'england': 'angleterre',\n",
       " 'include': 'inclut',\n",
       " 'association': 'association',\n",
       " 'down': 'descendre',\n",
       " 'given': 'donnés',\n",
       " 'source': 'sources',\n",
       " 'california': 'californienne',\n",
       " 'man': 'homme',\n",
       " 'version': 'version',\n",
       " 'written': 'écrits',\n",
       " 'created': 'créés',\n",
       " 'media': 'médias',\n",
       " 'black': 'noirs',\n",
       " 'php': 'php',\n",
       " 'report': 'rapport',\n",
       " 'building': 'bâtiment',\n",
       " 'take': 'prends',\n",
       " 'division': 'circonscription',\n",
       " 'comment': 'commenter',\n",
       " 'having': 'avoir',\n",
       " 'king': 'king',\n",
       " 'edit': 'éditer',\n",
       " 'stadium': 'stadium',\n",
       " 'died': 'mort',\n",
       " 'ship': 'vaisseau',\n",
       " 'research': 'recherche',\n",
       " 'record': 'enregistrer',\n",
       " 'archive': 'archives',\n",
       " 'places': 'lieux',\n",
       " 'undo': 'défaire',\n",
       " 'cup': 'coupe',\n",
       " 'records': 'records',\n",
       " 'often': 'souvent',\n",
       " 'few': 'peu',\n",
       " 'received': 'reçue',\n",
       " 'side': 'latéral',\n",
       " 'power': 'pouvoir',\n",
       " 'education': 'éducation',\n",
       " 'know': 'savoir',\n",
       " 'category': 'catégories',\n",
       " 'water': 'eau',\n",
       " 'species': 'espèce',\n",
       " 'field': 'domaine',\n",
       " 'near': 'près',\n",
       " 'australia': 'australie',\n",
       " 'video': 'video',\n",
       " 'need': 'besoin',\n",
       " 'island': 'île',\n",
       " 'form': 'formulaire',\n",
       " 'find': 'trouvez',\n",
       " 'served': 'desservis',\n",
       " 'play': 'jouez',\n",
       " 'project': 'projet',\n",
       " 'radio': 'radio',\n",
       " 'works': 'oeuvres',\n",
       " 'proposed': 'proposés',\n",
       " 'every': 'chaque',\n",
       " 'development': 'développement',\n",
       " 'example': 'exemple',\n",
       " 'live': 'vivre',\n",
       " 'union': 'syndicat',\n",
       " 'india': 'indes',\n",
       " 'next': 'next',\n",
       " 'special': 'spéciale',\n",
       " 'court': 'cour',\n",
       " 'region': 'région',\n",
       " 'little': 'petite',\n",
       " 'short': 'courtes',\n",
       " 'william': 'william',\n",
       " 'province': 'province',\n",
       " 'western': 'western',\n",
       " 'son': 'fiston',\n",
       " 'france': 'france',\n",
       " 'council': 'conseil',\n",
       " 'others': 'autres',\n",
       " 'royal': 'royaux',\n",
       " 'current': 'actuelle',\n",
       " 'street': 'rue',\n",
       " 'full': 'complet',\n",
       " 'red': 'rouge',\n",
       " 'too': 'too',\n",
       " 'department': 'département',\n",
       " 'san': 'san',\n",
       " 'help': 'aide',\n",
       " 'among': 'parmi',\n",
       " 'preserved': 'préservées',\n",
       " 'james': 'james',\n",
       " 'open': 'ouvrir',\n",
       " 'force': 'forcer',\n",
       " 'position': 'position',\n",
       " 'head': 'têtes',\n",
       " 'director': 'réalisateur',\n",
       " 'father': 'père',\n",
       " 'track': 'morceau',\n",
       " 'http': 'http',\n",
       " 'canada': 'canada',\n",
       " 'never': 'never',\n",
       " 'australian': 'australie',\n",
       " 'george': 'george',\n",
       " 'jpg': 'jpg',\n",
       " 'level': 'niveau',\n",
       " 'late': 'tard',\n",
       " 'summer': 'été',\n",
       " 'society': 'société',\n",
       " 'moved': 'déplacée',\n",
       " 'office': 'bureau',\n",
       " 'period': 'période',\n",
       " 'championship': 'championnat',\n",
       " 'round': 'ronds',\n",
       " 'story': 'récit',\n",
       " 'songs': 'chansons',\n",
       " 'various': 'diverses',\n",
       " 'file': 'fichier',\n",
       " 'days': 'journées',\n",
       " 'land': 'terres',\n",
       " 'business': 'entreprises',\n",
       " 'reason': 'raison',\n",
       " 'america': 'amérique',\n",
       " 'million': 'millions',\n",
       " 'european': 'européen',\n",
       " 'term': 'terme',\n",
       " 'six': 'six',\n",
       " 'post': 'publication',\n",
       " 'why': 'why',\n",
       " 'produced': 'produites',\n",
       " 'subject': 'sujet',\n",
       " 'young': 'jeune',\n",
       " 'total': 'totaux',\n",
       " 'david': 'david',\n",
       " 'science': 'sciences',\n",
       " 'related': 'liés',\n",
       " 'rock': 'rock',\n",
       " 'archived': 'archivés',\n",
       " 'railway': 'ferroviaire',\n",
       " 'become': 'devenir',\n",
       " 'led': 'led',\n",
       " 'students': 'élèves',\n",
       " 'started': 'commencée',\n",
       " 'news': 'actualités',\n",
       " 'described': 'décrite',\n",
       " 'role': 'rôle',\n",
       " 'election': 'élections',\n",
       " 'albums': 'albums',\n",
       " 'present': 'présenter',\n",
       " 'indian': 'indien',\n",
       " 'kingdom': 'royaume',\n",
       " 'books': 'livres',\n",
       " 'important': 'importants',\n",
       " 'northern': 'nord',\n",
       " 'love': 'love',\n",
       " 'run': 'exécuter',\n",
       " 'canadian': 'canadien',\n",
       " 'press': 'presse',\n",
       " 'rather': 'plutôt',\n",
       " 'type': 'tapez',\n",
       " 'act': 'act',\n",
       " 'editor': 'editeur',\n",
       " 'came': 'vint',\n",
       " 'schools': 'écoles',\n",
       " 'program': 'programme',\n",
       " 'once': 'once',\n",
       " 'social': 'social',\n",
       " 'germany': 'allemagne',\n",
       " 'production': 'production',\n",
       " 'male': 'homme',\n",
       " 'might': 'pourrait',\n",
       " 'awards': 'récompenses',\n",
       " 'points': 'points',\n",
       " 'similar': 'semblable',\n",
       " 'professional': 'professionnelles',\n",
       " 'say': 'dis',\n",
       " 'background': 'contexte',\n",
       " 'enough': 'assez',\n",
       " 'lead': 'plomb',\n",
       " 'either': 'soit',\n",
       " 'common': 'commun',\n",
       " 'overlap': 'chevauchements',\n",
       " 'data': 'données',\n",
       " 'color': 'couleurs',\n",
       " 'better': 'meilleur',\n",
       " 'person': 'personne',\n",
       " 'services': 'services',\n",
       " 'bgcolor': 'bgcolor',\n",
       " 'museum': 'musée',\n",
       " 'battle': 'combat',\n",
       " 'went': 'allé',\n",
       " 'sports': 'sports',\n",
       " 'already': 'dejà',\n",
       " 'currently': 'présentement',\n",
       " 'hall': 'hall',\n",
       " 'buildings': 'édifices',\n",
       " 'historic': 'historiques',\n",
       " 'date': 'date',\n",
       " 'deleted': 'supprimées',\n",
       " 'considered': 'considérés',\n",
       " 'change': 'changement',\n",
       " 'location': 'lieu',\n",
       " 'seems': 'semble',\n",
       " 'must': 'devez',\n",
       " 'yes': 'yes',\n",
       " 'our': 'nos',\n",
       " 'southern': 'méridionale',\n",
       " 'lost': 'perdus',\n",
       " 'something': 'quelquechose',\n",
       " 'review': 'révision',\n",
       " 'together': 'together',\n",
       " 'robert': 'thierry',\n",
       " 'less': 'moins',\n",
       " 'japanese': 'japonaises',\n",
       " 'groups': 'groupes',\n",
       " 'content': 'contenus',\n",
       " 'involved': 'impliquée',\n",
       " 'isbn': 'isbn',\n",
       " 'board': 'planche',\n",
       " 'japan': 'japon',\n",
       " 'control': 'contrôle',\n",
       " 'policy': 'politique',\n",
       " 'modern': 'modernes',\n",
       " 'human': 'humain',\n",
       " 'half': 'demi',\n",
       " 'design': 'conception',\n",
       " 'event': 'evénement',\n",
       " 'events': 'événements',\n",
       " 'available': 'disponibles',\n",
       " 'done': 'faite',\n",
       " 'washington': 'washington',\n",
       " 'real': 'vraie',\n",
       " 'start': 'début',\n",
       " 'personal': 'personnels',\n",
       " 'action': 'action',\n",
       " 'space': 'espace',\n",
       " 'areas': 'zones',\n",
       " 'star': 'étoiles',\n",
       " 'really': 'réellement',\n",
       " 'china': 'chine',\n",
       " 'possible': 'possibles',\n",
       " 'paul': 'paul',\n",
       " 'working': 'travailler',\n",
       " 'taken': 'prise',\n",
       " 'far': 'loin',\n",
       " 'going': 'aller',\n",
       " 'minister': 'ministre',\n",
       " 'lake': 'lac',\n",
       " 'reported': 'signalée',\n",
       " 'popular': 'populaire',\n",
       " 'married': 'marié',\n",
       " 'founded': 'fondée',\n",
       " 'europe': 'europe',\n",
       " 'author': 'auteur',\n",
       " 'away': 'loin',\n",
       " 'independent': 'indépendants',\n",
       " 'process': 'procédé',\n",
       " 'teams': 'equipes',\n",
       " 'character': 'personnage',\n",
       " 'low': 'bas',\n",
       " 'michael': 'michel',\n",
       " 'pages': 'pages',\n",
       " 'light': 'léger',\n",
       " 'big': 'grand',\n",
       " 'seen': 'vus',\n",
       " 'release': 'libèrent',\n",
       " 'want': 'voulez',\n",
       " 'episode': 'episode',\n",
       " 'wrote': 'écrit',\n",
       " 'republic': 'république',\n",
       " 'thomas': 'thomas',\n",
       " 'companies': 'sociétés',\n",
       " 'via': 'via',\n",
       " 'russian': 'russe',\n",
       " 'thanks': 'remerciements',\n",
       " 'put': 'mettre',\n",
       " 'race': 'race',\n",
       " 'worked': 'travaillé',\n",
       " 'route': 'parcours',\n",
       " 'recorded': 'enregistré',\n",
       " 'someone': 'someone',\n",
       " 'civil': 'civiles',\n",
       " 'police': 'policier',\n",
       " 'charles': 'charles',\n",
       " 'listed': 'répertoriés',\n",
       " 'users': 'utilisateurs',\n",
       " 'template': 'modèle',\n",
       " 'eastern': 'oriental',\n",
       " 'body': 'corps',\n",
       " 'question': 'question',\n",
       " 'italian': 'italiens',\n",
       " 'featured': 'vedettes',\n",
       " 'week': 'semaines',\n",
       " 'editors': 'éditeurs',\n",
       " 'texas': 'texas',\n",
       " 'chief': 'chef',\n",
       " 'close': 'proche',\n",
       " 'match': 'match',\n",
       " 'roman': 'romains',\n",
       " 'come': 'venir',\n",
       " 'opened': 'ouvert',\n",
       " 'tour': 'tournée',\n",
       " 'sea': 'mer',\n",
       " 'cross': 'croisé',\n",
       " 'playing': 'jouant',\n",
       " 'health': 'santé',\n",
       " 'institute': 'institut',\n",
       " 'caps': 'bouchons',\n",
       " 'forces': 'forces',\n",
       " 'green': 'vertes',\n",
       " 'rights': 'droits',\n",
       " 'evidence': 'preuves',\n",
       " 'originally': 'initialement',\n",
       " 'aircraft': 'avions',\n",
       " 'arts': 'arts',\n",
       " 'range': 'portée',\n",
       " 'probably': 'sûrement',\n",
       " 'consensus': 'consensus',\n",
       " 'bar': 'barre',\n",
       " 'problem': 'problématique',\n",
       " 'look': 'regardes',\n",
       " 'issues': 'problèmes',\n",
       " 'alumni': 'anciens',\n",
       " 'average': 'moyenne',\n",
       " 'network': 'réseau',\n",
       " 'win': 'gagnant',\n",
       " 'shows': 'spectacles',\n",
       " 'wife': 'femme',\n",
       " 'returned': 'retournée',\n",
       " 'night': 'soir',\n",
       " 'magazine': 'magasine',\n",
       " 'centre': 'centre',\n",
       " 'joined': 'rejoint',\n",
       " 'usually': 'généralement',\n",
       " 'middle': 'milieu',\n",
       " 'completed': 'terminé',\n",
       " 'elected': 'élus',\n",
       " 'significant': 'significatifs',\n",
       " 'african': 'africaines',\n",
       " 'able': 'capable',\n",
       " 'google': 'google',\n",
       " 'stage': 'scène',\n",
       " 'addition': 'ajout',\n",
       " 'ireland': 'irlande',\n",
       " 'today': 'aujourdhui',\n",
       " 'academy': 'academy',\n",
       " 'saint': 'saint',\n",
       " 'self': 'self',\n",
       " 'itself': 'soi',\n",
       " 'continued': 'continué',\n",
       " 'stations': 'stations',\n",
       " 'mother': 'maman',\n",
       " 'appeared': 'semblait',\n",
       " 'africa': 'afrique',\n",
       " 'culture': 'culture',\n",
       " 'spanish': 'espagnols',\n",
       " 'grand': 'grand',\n",
       " 'committee': 'comité',\n",
       " 'things': 'choses',\n",
       " 'fire': 'incendies',\n",
       " 'changed': 'changée',\n",
       " 'gold': 'gold',\n",
       " 'female': 'femme',\n",
       " 'course': 'cours',\n",
       " 'directed': 'orienté',\n",
       " 'months': 'mois',\n",
       " 'chinese': 'chinoise',\n",
       " 'previous': 'précédents',\n",
       " 'developed': 'développé',\n",
       " 'size': 'tailles',\n",
       " 'mentioned': 'mentionnés',\n",
       " 'add': 'ajoutez',\n",
       " 'festival': 'fête',\n",
       " 'peter': 'pierre',\n",
       " 'basketball': 'basketball',\n",
       " 'move': 'déplacer',\n",
       " 'performance': 'rendement',\n",
       " 'standard': 'standard',\n",
       " 'means': 'signifie',\n",
       " 'give': 'donnez',\n",
       " 'training': 'entraînement',\n",
       " 'artist': 'artistes',\n",
       " 'word': 'mot',\n",
       " 'blue': 'bleues',\n",
       " 'primary': 'primaires',\n",
       " 'announced': 'annoncée',\n",
       " 'value': 'valeur',\n",
       " 'christian': 'chrétien',\n",
       " 'private': 'privés',\n",
       " 'catholic': 'catholiques',\n",
       " 'artists': 'artistes',\n",
       " 'includes': 'comprend',\n",
       " 'view': 'visualiser',\n",
       " 'thus': 'ainsi',\n",
       " 'almost': 'quasiment',\n",
       " 'baseball': 'baseball',\n",
       " 'seven': 'sept',\n",
       " 'appears': 'apparait',\n",
       " 'ever': 'ever',\n",
       " 'provide': 'fournissent',\n",
       " 'technology': 'technologie',\n",
       " 'olympics': 'olympiques',\n",
       " 'future': 'avenir',\n",
       " 'formed': 'formée',\n",
       " 'census': 'recensement',\n",
       " 'images': 'images',\n",
       " 'los': 'los',\n",
       " 'results': 'résultats',\n",
       " 'return': 'revenir',\n",
       " 'quality': 'qualité',\n",
       " 'construction': 'construction',\n",
       " 'zealand': 'zélande',\n",
       " 'front': 'devant',\n",
       " 'cover': 'housse',\n",
       " 'model': 'maquette',\n",
       " 'despite': 'malgré',\n",
       " 'read': 'lis',\n",
       " 'material': 'matériau',\n",
       " 'strong': 'fort',\n",
       " 'coach': 'entraîneur',\n",
       " 'henry': 'henry',\n",
       " 'footballers': 'footballeurs',\n",
       " 'mark': 'mark',\n",
       " 'rev': 'rev',\n",
       " 'organization': 'organisation',\n",
       " 'studies': 'études',\n",
       " 'federal': 'fédérales',\n",
       " 'richard': 'richard',\n",
       " 'html': 'html',\n",
       " 'virginia': 'virginia',\n",
       " 'car': 'voiture',\n",
       " 'attack': 'attaquer',\n",
       " 'conference': 'conférence',\n",
       " 'outside': 'exterieur',\n",
       " 'study': 'étude',\n",
       " 'brother': 'frere',\n",
       " 'names': 'noms',\n",
       " 'writer': 'scénariste',\n",
       " 'characters': 'caractères',\n",
       " 'musical': 'musical',\n",
       " 'nothing': 'rien',\n",
       " 'border': 'bordure',\n",
       " 'medical': 'médical',\n",
       " 'countries': 'pays',\n",
       " 'past': 'passés',\n",
       " 'writing': 'écrire',\n",
       " 'makes': 'rend',\n",
       " 'interest': 'intérêt',\n",
       " 'provided': 'fournis',\n",
       " 'killed': 'tués',\n",
       " 'medal': 'médailles',\n",
       " 'signed': 'signés',\n",
       " 'label': 'étiquettes',\n",
       " 'fair': 'équitables',\n",
       " 'search': 'recherchez',\n",
       " 'bay': 'bay',\n",
       " 'reference': 'référence',\n",
       " 'especially': 'spécialement',\n",
       " 'removed': 'supprimé',\n",
       " 'library': 'librairie',\n",
       " 'eventually': 'finalement',\n",
       " 'management': 'gestion',\n",
       " 'references': 'références',\n",
       " 'features': 'fonctionnalités',\n",
       " 'navy': 'marine',\n",
       " 'guitar': 'guitares',\n",
       " 'hill': 'colline',\n",
       " 'sure': 'sûr',\n",
       " 'historical': 'historiques',\n",
       " 'lower': 'inférieure',\n",
       " 'daughter': 'fille',\n",
       " 'appointed': 'nommés',\n",
       " 'reading': 'lire',\n",
       " 'yet': 'pourtant',\n",
       " 'systems': 'systèmes',\n",
       " 'debut': 'débuts',\n",
       " 'movement': 'mouvement',\n",
       " 'specific': 'spécifique',\n",
       " 'always': 'toujours',\n",
       " 'actor': 'acteur',\n",
       " 'natural': 'naturelle',\n",
       " 'clear': 'effacer',\n",
       " 'coast': 'côte',\n",
       " 'let': 'let',\n",
       " 'got': 'got',\n",
       " 'chicago': 'chicago',\n",
       " 'championships': 'championnats',\n",
       " 'pennsylvania': 'pennsylvanie',\n",
       " 'ten': 'ten',\n",
       " 'performed': 'effectué',\n",
       " 'individual': 'individuel',\n",
       " 'designed': 'conçus',\n",
       " 'rule': 'règle',\n",
       " 'etc': 'etc',\n",
       " 'lists': 'listes',\n",
       " 'paris': 'paris',\n",
       " 'thought': 'pensée',\n",
       " 'brown': 'brune',\n",
       " 'hand': 'hand',\n",
       " 'needs': 'besoins',\n",
       " 'reliable': 'fiables',\n",
       " 'smith': 'forgeron',\n",
       " 'generally': 'généralement',\n",
       " 'base': 'base',\n",
       " 'sometimes': 'parfois',\n",
       " 'florida': 'floride',\n",
       " 'capital': 'capital',\n",
       " 'valley': 'vallée',\n",
       " 'bank': 'banque',\n",
       " 'ground': 'moulu',\n",
       " 'reached': 'atteint',\n",
       " 'italy': 'italie',\n",
       " 'energy': 'energie',\n",
       " 'believe': 'crois',\n",
       " 'leader': 'leader',\n",
       " 'active': 'actifs',\n",
       " 'online': 'online',\n",
       " 'block': 'blocage',\n",
       " 'bridge': 'passerelle',\n",
       " 'families': 'familles',\n",
       " 'changes': 'changements',\n",
       " 'followed': 'suivies',\n",
       " 'industry': 'industrie',\n",
       " 'collection': 'collecte',\n",
       " 'request': 'demandez',\n",
       " 'soon': 'bientot',\n",
       " 'olympic': 'olympiques',\n",
       " 'sold': 'vendu',\n",
       " 'writers': 'écrivains',\n",
       " 'professor': 'professeure',\n",
       " 'studio': 'studio',\n",
       " 'mexico': 'mexique',\n",
       " 'competition': 'concours',\n",
       " 'campaign': 'campagne',\n",
       " 'org': 'org',\n",
       " 'theatre': 'théâtres',\n",
       " 'particular': 'particulier',\n",
       " 'empire': 'empire',\n",
       " 'length': 'longueurs',\n",
       " 'islands': 'iles',\n",
       " 'singer': 'chanteuse',\n",
       " 'create': 'créent',\n",
       " 'redirect': 'réorienter',\n",
       " 'additional': 'additionnel',\n",
       " 'soviet': 'soviétiques',\n",
       " 'market': 'marché',\n",
       " 'words': 'mots',\n",
       " 'producer': 'producteurs',\n",
       " 'notes': 'notes',\n",
       " 'hockey': 'hockey',\n",
       " 'code': 'code',\n",
       " 'referee': 'arbitre',\n",
       " 'fourth': 'quatrièmement',\n",
       " 'sport': 'sport',\n",
       " 'van': 'van',\n",
       " 'mary': 'myriam',\n",
       " 'airport': 'aéroport',\n",
       " 'sound': 'son',\n",
       " 'status': 'statut',\n",
       " 'irish': 'irlandais',\n",
       " 'placed': 'placé',\n",
       " 'child': 'enfant',\n",
       " 'idea': 'idée',\n",
       " 'foreign': 'étrangères',\n",
       " 'municipality': 'municipalité',\n",
       " 'register': 'registre',\n",
       " 'eight': 'eight',\n",
       " 'problems': 'problèmes',\n",
       " 'native': 'autochtone',\n",
       " 'coverage': 'couverture',\n",
       " 'channel': 'channel',\n",
       " 'parliament': 'parlement',\n",
       " 'username': 'pseudo',\n",
       " 'edition': 'édition',\n",
       " 'minor': 'mineur',\n",
       " 'says': 'dit',\n",
       " 'foundation': 'fondations',\n",
       " 'units': 'unités',\n",
       " 'movie': 'film',\n",
       " 'ice': 'glace',\n",
       " 'simply': 'simplement',\n",
       " 'limited': 'limités',\n",
       " 'unit': 'unit',\n",
       " 'student': 'etudiant',\n",
       " 'previously': 'précédemment',\n",
       " 'stated': 'déclaré',\n",
       " 'governor': 'gouverneur',\n",
       " 'complete': 'complet',\n",
       " 'test': 'tester',\n",
       " 'nominated': 'désignés',\n",
       " 'bill': 'facturer',\n",
       " 'parts': 'parties',\n",
       " 'vocals': 'voix',\n",
       " 'theory': 'théorie',\n",
       " 'regional': 'régional',\n",
       " 'account': 'compte',\n",
       " 'vote': 'voter',\n",
       " 'computer': 'ordinateur',\n",
       " 'none': 'aucune',\n",
       " 'carolina': 'carolina',\n",
       " 'tournament': 'tournoi',\n",
       " 'poland': 'pologne',\n",
       " 'behind': 'derrière',\n",
       " 'wales': 'galles',\n",
       " 'winning': 'gagnant',\n",
       " 'lot': 'lot',\n",
       " 'hospital': 'hôpitaux',\n",
       " 'mid': 'mid',\n",
       " 'taking': 'prenant',\n",
       " 'mountain': 'montagnes',\n",
       " 'higher': 'supérieur',\n",
       " 'cases': 'cas',\n",
       " 'angeles': 'angeles',\n",
       " 'editing': 'édition',\n",
       " 'replaced': 'remplacés',\n",
       " 'food': 'alimentation',\n",
       " 'multiple': 'plusieurs',\n",
       " 'likely': 'probablement',\n",
       " 'terms': 'termes',\n",
       " 'sir': 'monsieur',\n",
       " 'thing': 'chose',\n",
       " 'square': 'carrées',\n",
       " 'try': 'essaye',\n",
       " 'topic': 'sujet',\n",
       " 'woman': 'femme',\n",
       " 'officer': 'officier',\n",
       " 'categories': 'catégories',\n",
       " 'greek': 'grec',\n",
       " 'recent': 'récente',\n",
       " 'sent': 'envoyé',\n",
       " 'copyright': 'copyright',\n",
       " 'speed': 'vitesse',\n",
       " 'templates': 'gabarits',\n",
       " 'money': 'argent',\n",
       " 'saw': 'scie',\n",
       " 'senior': 'senior',\n",
       " 'selected': 'sélectionné',\n",
       " 'introduced': 'introduites',\n",
       " 'politician': 'politicien',\n",
       " 'true': 'véritable',\n",
       " 'required': 'requis',\n",
       " 'regular': 'régulier',\n",
       " 'awarded': 'décernés',\n",
       " 'commercial': 'commerciale',\n",
       " 'cities': 'villes',\n",
       " 'contains': 'contient',\n",
       " 'trade': 'échanges',\n",
       " 'degree': 'degré',\n",
       " 'anti': 'anti',\n",
       " 'birth': 'naissance',\n",
       " 'sun': 'soleil',\n",
       " 'finished': 'terminé',\n",
       " 'rugby': 'rugby',\n",
       " 'earth': 'terre',\n",
       " 'access': 'accéder',\n",
       " 'prior': 'prieur',\n",
       " 'seasons': 'saisons',\n",
       " 'journal': 'journal',\n",
       " 'beginning': 'début',\n",
       " 'software': 'logiciels',\n",
       " 'famous': 'célèbres',\n",
       " 'religious': 'religieuse',\n",
       " 'appear': 'apparaissent',\n",
       " 'martin': 'martin',\n",
       " 'god': 'dieu',\n",
       " 'bit': 'bits',\n",
       " 'hours': 'heures',\n",
       " 'running': 'courir',\n",
       " 'brought': 'amenés',\n",
       " 'missing': 'disparu',\n",
       " 'economic': 'economique',\n",
       " 'structure': 'structure',\n",
       " 'rural': 'rural',\n",
       " 'remained': 'restait',\n",
       " 'decision': 'décision',\n",
       " 'certain': 'certaines',\n",
       " 'hit': 'frapper',\n",
       " 'minutes': 'minutes',\n",
       " 'spain': 'espagne',\n",
       " 'plays': 'joue',\n",
       " 'whole': 'entier',\n",
       " 'joseph': 'joseph',\n",
       " 'lord': 'seigneur',\n",
       " 'web': 'enchaînement',\n",
       " 'decided': 'décidé',\n",
       " 'operations': 'opérations',\n",
       " 'function': 'fonction',\n",
       " 'louis': 'louis',\n",
       " 'assembly': 'assemblage',\n",
       " 'queen': 'queen',\n",
       " 'security': 'sécurité',\n",
       " 'uses': 'utilisations',\n",
       " ...}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_fr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_fr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    english_set = set(english_vecs.keys())\n",
    "    french_set = set(french_vecs.keys())\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "            en_vec = english_vecs[en_word]\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            X_l.append(en_vec)\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    X = np.array(X_l)\n",
    "    print(X.shape)\n",
    "\n",
    "    Y = np.array(Y_l)\n",
    "    print(Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4932, 300)\n",
      "(4932, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:57: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4932, 300)\n",
      "(4932, 300)\n",
      "(1438, 300)\n",
      "(1438, 300)\n",
      "\u001b[92m All tests passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "/Users/mikael/Library/Mobile Documents/com~apple~CloudDocs/Learning/Deeplearning.ai/Specializations/003-nlp/001-nlp-with-classification-and-vector-spaces/assingments/004-week-4-machine-translation/utils.py:57: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_get_matrices(get_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    diff = X @ R - Y\n",
    "    diff_squared = np.square(diff)\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    loss = sum_diff_squared / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for an experiment with random matrices: 8.1866\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * 0.1\n",
    "R = np.random.rand(n, n)\n",
    "print(\n",
    "    f\"Expected loss for an experiment with random matrices: {compute_loss(X, Y, R):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradiant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "\n",
    "    gradient = X.T @ (X @ R - Y) * (2 / m)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the gradient matrix: [1.3498175  1.11264981 0.69626762 0.98468499 1.33828969]\n"
     ]
    }
   ],
   "source": [
    "# Testing your implementation.\n",
    "np.random.seed(123)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * 0.1\n",
    "R = np.random.rand(n, n)\n",
    "gradient = compute_gradient(X, Y, R)\n",
    "print(f\"First row of the gradient matrix: {gradient[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_compute_gradient(compute_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(\n",
    "    X,\n",
    "    Y,\n",
    "    train_steps=100,\n",
    "    learning_rate=0.0003,\n",
    "    verbose=True,\n",
    "    compute_loss=compute_loss,\n",
    "    compute_gradient=compute_gradient,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings. ---> Rows m words, Cols n embeedding features\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    \"\"\"\n",
    "    np.random.seed(129)\n",
    "\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f\"Loss at iteration {i} i {compute_loss(X, Y, R)}\")\n",
    "\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "        R = R - learning_rate * gradient\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 i 3.7241601261682264\n",
      "Loss at iteration 25 i 3.628289198210137\n",
      "Loss at iteration 50 i 3.53497687566553\n",
      "Loss at iteration 75 i 3.444154562048557\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * 0.1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_align_embeddings(align_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 i 963.0145619955867\n",
      "Loss at iteration 25 i 97.82916853030906\n",
      "Loss at iteration 50 i 26.832879686583038\n",
      "Loss at iteration 75 i 9.789329539535158\n",
      "Loss at iteration 100 i 4.377647478540068\n",
      "Loss at iteration 125 i 2.3280536915224053\n",
      "Loss at iteration 150 i 1.4479896206566953\n",
      "Loss at iteration 175 i 1.0337809042840693\n",
      "Loss at iteration 200 i 0.8251343300366362\n",
      "Loss at iteration 225 i 0.7144881382598546\n",
      "Loss at iteration 250 i 0.6533957223255501\n",
      "Loss at iteration 275 i 0.6185348691701879\n",
      "Loss at iteration 300 i 0.5980814008397813\n",
      "Loss at iteration 325 i 0.5857882199933446\n",
      "Loss at iteration 350 i 0.578241077375853\n",
      "Loss at iteration 375 i 0.5735195038803573\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the translation with K-nearest neighbours algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    similarity_l = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        similarity = cosine_similarity(candidate, v)\n",
    "        similarity_l.append(similarity)\n",
    "\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    k_idx = sorted_ids[::-1][:k]\n",
    "\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  5]\n",
      " [-2  5  3]\n",
      " [ 2  0  1]\n",
      " [ 6 -9  5]\n",
      " [ 9  9  9]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation:\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates)\n",
    "# print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_nearest_neighbor(nearest_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your translation and compute its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # The prediction is X times R\n",
    "    print(f\"Shape X: {X.shape}\")\n",
    "    print(f\"Shape Y: {Y.shape}\")\n",
    "    print(f\"Shape R: {R.shape}\")\n",
    "    pred = X @ R\n",
    "    print(f\"Shape pred: {pred.shape}\")\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    print(f\"Len pred: {len(pred)}\")\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(v=pred[i, :], candidates=Y)\n",
    "\n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / pred.shape[0]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1438, 300)\n",
      "(1438, 300)\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (1438, 300)\n",
      "Shape Y: (1438, 300)\n",
      "Shape R: (300, 300)\n",
      "Shape pred: (1438, 300)\n",
      "Len pred: 1438\n",
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(\n",
    "    X,\n",
    "    Y,\n",
    "    train_steps=100,\n",
    "    learning_rate=0.0003,\n",
    "    verbose=True,\n",
    "    compute_loss=compute_loss,\n",
    "    compute_gradient=compute_gradient,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings. ---> Rows m words, Cols n embeedding features\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    \"\"\"\n",
    "    np.random.seed(129)\n",
    "\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print(f\"Loss at iteration {i} i {compute_loss(X, Y, R)}\")\n",
    "\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "        R = R - learning_rate * gradient\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 i 3.7241601261682264\n",
      "Loss at iteration 25 i 3.628289198210137\n",
      "Loss at iteration 50 i 3.53497687566553\n",
      "Loss at iteration 75 i 3.444154562048557\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * 0.1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_align_embeddings(align_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 i 963.0145619955867\n",
      "Loss at iteration 25 i 97.82916853030906\n",
      "Loss at iteration 50 i 26.832879686583038\n",
      "Loss at iteration 75 i 9.789329539535158\n",
      "Loss at iteration 100 i 4.377647478540068\n",
      "Loss at iteration 125 i 2.3280536915224053\n",
      "Loss at iteration 150 i 1.4479896206566953\n",
      "Loss at iteration 175 i 1.0337809042840693\n",
      "Loss at iteration 200 i 0.8251343300366362\n",
      "Loss at iteration 225 i 0.7144881382598546\n",
      "Loss at iteration 250 i 0.6533957223255501\n",
      "Loss at iteration 275 i 0.6185348691701879\n",
      "Loss at iteration 300 i 0.5980814008397813\n",
      "Loss at iteration 325 i 0.5857882199933446\n",
      "Loss at iteration 350 i 0.578241077375853\n",
      "Loss at iteration 375 i 0.5735195038803573\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the translation with K-nearest neighbours algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    similarity_l = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        similarity = cosine_similarity(candidate, v)\n",
    "        similarity_l.append(similarity)\n",
    "\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    k_idx = sorted_ids[::-1][:k]\n",
    "\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  5]\n",
      " [-2  5  3]\n",
      " [ 2  0  1]\n",
      " [ 6 -9  5]\n",
      " [ 9  9  9]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation:\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates)\n",
    "# print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_nearest_neighbor(nearest_neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your translation and compute its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # The prediction is X times R\n",
    "    print(f\"Shape X: {X.shape}\")\n",
    "    print(f\"Shape Y: {Y.shape}\")\n",
    "    print(f\"Shape R: {R.shape}\")\n",
    "    pred = X @ R\n",
    "    print(f\"Shape pred: {pred.shape}\")\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    print(f\"Len pred: {len(pred)}\")\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(v=pred[i, :], candidates=Y)\n",
    "\n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / pred.shape[0]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1438, 300)\n",
      "(1438, 300)\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (1438, 300)\n",
      "Shape Y: (1438, 300)\n",
      "Shape R: (300, 300)\n",
      "Shape pred: (1438, 300)\n",
      "Len pred: 1438\n",
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (2, 1)\n",
      "Shape Y: (2, 1)\n",
      "Shape R: (1, 1)\n",
      "Shape pred: (2, 1)\n",
      "Len pred: 2\n",
      "Shape X: (4, 2)\n",
      "Shape Y: (4, 2)\n",
      "Shape R: (2, 2)\n",
      "Shape pred: (4, 2)\n",
      "Len pred: 4\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.unittest_test_vocabulary(test_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH and Document Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "all_negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embeddings, process_tweet=process_tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    \"\"\"\n",
    "\n",
    "    doc_embedding = np.zeros(300)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "\n",
    "    for word in processed_doc:\n",
    "        doc_embedding += en_embeddings.get(word, 0)\n",
    "\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_get_document_embedding(get_document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(\n",
    "    all_docs, en_embeddings, get_document_embedding=get_document_embedding\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in our dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - document_vec_matrix: matrix of tweet embeddings.\n",
    "        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    \"\"\"\n",
    "    ind2Doc_dict = {}\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "        document_embedding = get_document_embedding(doc, en_embeddings)\n",
    "        document_vec_l.append(document_embedding)\n",
    "        ind2Doc_dict[i] = document_embedding\n",
    "\n",
    "    document_vec_matrix = np.array(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_get_document_vecs(get_document_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = \"i am sad\"\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12436915, 0.34844389, 0.23104973, ..., 0.        , 0.13630579,\n",
       "       0.09393305])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = cosine_similarity(document_vecs, tweet_embedding)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5202"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.argmax(similarity)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@hanbined sad pray for me :((('"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most similar tweet with LSH\n",
    "\n",
    "You will now implement locality sensitive hashing (LSH) to identify the most similar tweet.\n",
    "* Instead of looking at all 10,000 vectors, you can just search a subset to find\n",
    "its nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)  # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])  # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the number of planes\n",
    "\n",
    "* Each plane divides the space to $2$ parts.\n",
    "* So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "* We want to organize 10,000 document vectors into buckets so that every bucket has about $~16$ vectors.\n",
    "* For that we need $\\frac{10000}{16}=625$ buckets.\n",
    "* We're interested in $n$, number of planes, so that $2^{n}= 625$. Now, we can calculate $n=\\log_{2}625 = 9.29 \\approx 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES)) for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # for the set of planes,\n",
    "    # calculate the dot product between the vector and the matrix containing the planes\n",
    "    # remember that planes has shape (300, 10)\n",
    "    # The dot product will have the shape (1,10)\n",
    "    # planes: [N_DIMS, N_PLANES]\n",
    "    # v: [1 , N_DIMS]\n",
    "    # planes x v.T -> [N_DIMS, N_PLANES] x [N_DIMS, 1]\n",
    "    dot_product = np.dot(v, planes)\n",
    "\n",
    "    # get the sign of the dot product (1,10) shaped vector\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "\n",
    "    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n",
    "    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n",
    "    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive\n",
    "    h = np.where(sign_of_dot_product >= 0, 1, 0)\n",
    "\n",
    "    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n",
    "    h = np.squeeze(h)\n",
    "\n",
    "    # initialize the hash value to 0\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]  # N_PLANES\n",
    "    for i in range(n_planes):\n",
    "        # increment the hash value by 2^i * h_i\n",
    "        hash_value += 2**i * h[i]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # cast hash_value as an integer\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The hash value for this vector, and the set of planes at index 0, is 768\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "print(\n",
    "    f\" The hash value for this vector,\",\n",
    "    f\"and the set of planes at index {idx},\",\n",
    "    f\"is {hash_value_of_vector(vec, planes)}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w4_unittest.test_hash_value_of_vector(hash_value_of_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes, hash_value_of_vector=hash_value_of_vector):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    # ALTERNATIVE SOLUTION COMMENT:\n",
    "    # num_buckets = pow(2, num_of_planes)\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {i: [] for i in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {i: [] for i in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)  # @REPLACE None\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)  # @REPLACE None\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 3 document vectors\n",
      "The id table at key 0 has 3 document indices\n",
      "The first 5 document indices stored at key 0 of id table are [3276, 3281, 3282]\n"
     ]
    }
   ],
   "source": [
    "planes = planes_l[0]  # get one 'universe' of planes to test the function\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])} document indices\")\n",
    "print(\n",
    "    f\"The first 5 document indices stored at key 0 of id table are {tmp_id_table[0][0:5]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "def create_hash_id_tables(n_universes):\n",
    "    hash_tables = []\n",
    "    id_tables = []\n",
    "    for universe_id in range(n_universes):  # there are 25 hashes\n",
    "        print(\"working on hash universe #:\", universe_id)\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "        hash_tables.append(hash_table)\n",
    "        id_tables.append(id_table)\n",
    "\n",
    "    return hash_tables, id_tables\n",
    "\n",
    "\n",
    "hash_tables, id_tables = create_hash_id_tables(N_UNIVERSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(\n",
    "    doc_id,\n",
    "    v,\n",
    "    planes_l,\n",
    "    hash_tables,\n",
    "    id_tables,\n",
    "    k=1,\n",
    "    num_universes_to_use=25,\n",
    "    hash_value_of_vector=hash_value_of_vector,\n",
    "):\n",
    "    \"\"\"Search for k-NN using hashes.\"\"\"\n",
    "    # assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    # Vectors that will be checked as possible nearest neighbor\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    # list of document IDs\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    # loop through the universes of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "        # get the set of planes from the planes_l list, for this particular universe_id\n",
    "        planes = planes_l[universe_id]\n",
    "\n",
    "        # get the hash value of the vector for this set of planes\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # get the hash table for this particular universe_id\n",
    "        hash_table = hash_tables[universe_id]\n",
    "\n",
    "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # get the id_table for this particular universe_id\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "            if doc_id == new_id:\n",
    "                continue\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                ids_to_consider_l.append(new_id)\n",
    "\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx] for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 77 vecs\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, hash_tables, id_tables, k=3, num_universes_to_use=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n",
      "Nearest neighbor at document id 2478\n",
      "document contents: #ShareTheLove @oymgroup @musicartisthere for being top HighValue members this week :) @nataliavas http://t.co/IWSDMtcayt\n",
      "Nearest neighbor at document id 105\n",
      "document contents: #FollowFriday @straz_das @DCarsonCPA @GH813600 for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
